{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, itertools, csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pytrends.request import TrendReq\n",
    "from datetime import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content\n",
       "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
       "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
       "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
       "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
       "4   4          -1  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./data/train.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA's Grand Challenge: Stop Asteroids From Destroying Earth\n"
     ]
    }
   ],
   "source": [
    "def get_title(text: str) -> str:    \n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    title = soup.find('h1',{'class':'title'})\n",
    "    return title.get_text()\n",
    "# test\n",
    "print(get_title(df['Page content'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['climate', 'u.s.', 'world', 'warmest year']\n"
     ]
    }
   ],
   "source": [
    "def get_category(text: str) -> list:\n",
    "    res = list()\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cats = soup.find_all('a', {'href': re.compile('/category/*')})\n",
    "    for cat in cats:\n",
    "        res.append(cat.get_text().lower())\n",
    "    return res\n",
    "        \n",
    "# test\n",
    "print(get_category(df['Page content'][2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013-06-20', '15:04:30']\n"
     ]
    }
   ],
   "source": [
    "def get_date(text: str) -> datetime:\n",
    "    soup = BeautifulSoup(text)\n",
    "    selector = \"time\"\n",
    "    date = [i.text for i in soup.select(selector)][0]\n",
    "    date = date.split()\n",
    "    date = str(date[0] + ' ' + date[1])\n",
    "\n",
    "    d = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    return d\n",
    "\n",
    "# test\n",
    "print(str(get_date(df['Page content'][0]) + timedelta(days=1)).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " clara moskowitz for space com 2013 06 19 15 04 30 utc nasa s grand challenge stop asteroids from destroying earth there may be killer asteroids headed for earth and nasa has decided to do something about it the space agency announced a new grand challenge on june 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet the new mission builds on projects already underway at nasa including a plan to capture an asteroid pull it in toward the moon and send astronauts to visit it as part of the grand challenge the agency issued a request for information today aiming to solicit ideas from industry academia and the public on how to improve the asteroid mission plan we re asking for you to think about concepts and different approaches for what we ve described here william gerstenmaier nasa s associate administrator for human explorations and operations said yesterday during a nasa event announcing the initiative we want you to think about other ways of enhancing this to get the most out of it see also how it works nasa asteroid captureresponses to the request for information which also seeks ideas for detecting and mitigating asteroid threats are due july 18 the asteroid retrieval mission designed to provide the first deep space mission for astronauts flying on nasa s space launch system rocket and orion space capsule under development has come under fire from lawmakers who would prefer that nasa return to the moon a draft nasa authorization bill from the house space subcommittee which is currently in debate would cancel the mission and steer the agency toward other projects that bill will be discussed during a hearing wednesday june 19 at 10 a m edt see also how it works nasa asteroid capture mission in picturesbut nasa officials defended the asteroid mission today and said they were confident they d win congress support once they explained its benefits further i think that we really truly are going to be able to show the value of the mission nasa associate administrator lori garver said today to me this is something that what we do in this country we debate how we spend the public s money this is the beginning of the debate garver also maintained that sending astronauts to an asteroid would not diminish nasa s other science and exploration goals including another lunar landing see also animation of proposed asteroid retrieval mission this initiative takes nothing from the other valuable work she said this is only a small piece of our overall strategy but it is an integral piece it takes nothing from the moon part of nasa s plan to win support for the flight is to link it more closely with the larger goal of protecting earth from asteroid threats if someday humanity discovers an asteroid headed for earth and manages to alter its course it will be one of the most important accomplishments in human history said tom kalil deputy director for technology and innovation at the white house office of science and technology policy see also wildest private deep space mission ideas a countdownthe topic of asteroid threats is more timely than ever after a meteor exploded over the russian city of chelyabinsk on feb 15 the same day that the football field sized asteroid 2012 da14 passed within the moon s orbit of earth image courtesy of nasa spacex s musk says sabotage unlikely cause of sept 1 explosion but still a worry proxima centauri is like our sun on steroids china launches shenzhou 11 astronauts to tiangong 2 space lab space station mockup in houston astronaut guided tour video this article originally published at space com here topics asteroid asteroids challenge earth space u s world  \n"
     ]
    }
   ],
   "source": [
    "def preprocessor(text: str) -> str:\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "# test\n",
    "print(preprocessor(df['Page content'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nasa', \"'s\", 'grand', 'challenge', ':', 'stop', 'asteroid', 'destroying', 'earth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yuwei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yuwei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem(text: str) -> list:\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "#     stemmer = nltk.stem.PorterStemmer()\n",
    "#     \" \".join(stemmer.stem(token) for token in tokens)\n",
    "    \n",
    "    stemmer = nltk.stem.WordNetLemmatizer()\n",
    "    # text =  \" \".join(stemmer.lemmatize(token) for token in tokens)\n",
    "    \n",
    "    return [stemmer.lemmatize(token) for token in tokens if token not in stop]\n",
    "\n",
    "# test\n",
    "print(tokenizer_stem(get_title(df['Page content'][0]).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[example documents]\n",
      "Study hard, then you will be happy and I will be happy\n",
      "\"I'm not happy :(\" \", because you don't study hard\n",
      "\n",
      "[vocabulary]\n",
      "{'study': 9, 'hard': 6, 'happy': 3, 'study hard': 10, 'hard happy': 8, 'happy happy': 4, ':': 1, '(': 0, 'happy study': 5, 'hard :': 7, ': (': 2}\n",
      "(did, vid)\ttf\n",
      "  (0, 3)\t2\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "\n",
      "Is document-term matrix a scipy.sparse matrix? True\n",
      "[[0 0 0 2 1 0 1 0 1 1 1]\n",
      " [1 1 1 1 0 1 1 1 0 1 1]]\n",
      "\n",
      "After calling .toarray(), is it a scipy.sparse matrix? False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "doc_dummy = [\"Study hard, then you will be happy and I will be happy\", \n",
    "           \"\\\"I'm not happy :(\\\" \\\", because you don't study hard\"]\n",
    "print('[example documents]\\n{}\\n'.format('\\n'.join(doc_dummy)))\n",
    "\n",
    "# ngram_range=(min,max), default: 1-gram => (1,1)\n",
    "count = CountVectorizer(ngram_range=(1, 2),\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem)\n",
    "\n",
    "count.fit(doc_dummy)\n",
    "# dictionary is stored in vocabulary_\n",
    "BoW = count.vocabulary_\n",
    "print('[vocabulary]\\n{}'.format(BoW))\n",
    "\n",
    "# get matrix (doc_id, vocabulary_id) --> tf\n",
    "doc_bag = count.transform(doc_dummy)\n",
    "print('(did, vid)\\ttf')\n",
    "print(doc_bag)\n",
    "\n",
    "print('\\nIs document-term matrix a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))\n",
    "\n",
    "doc_bag = doc_bag.toarray()\n",
    "print(doc_bag)\n",
    "\n",
    "print('\\nAfter calling .toarray(), is it a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            NFV  SDN  google  ocean  car  isPartial\n",
      "date                                               \n",
      "2019-06-20    0    0      98      3   23      False\n",
      "2019-06-21    0    0      96      3   24      False\n",
      "2019-06-22    0    0      83      3   25      False\n",
      "2019-06-23    0    0      84      3   25      False\n",
      "2019-06-24    0    0      98      3   23      False\n",
      "            NFV  SDN  google  ocean  car  isPartial\n",
      "date                                               \n",
      "2019-06-20    0    0      98      3   23      False\n",
      "2019-06-21    0    0      96      3   24      False\n",
      "2019-06-22    0    0      83      3   25      False\n",
      "2019-06-23    0    0      84      3   25      False\n",
      "2019-06-24    0    0      98      3   23      False\n",
      "2019-06-25    0    1     100      3   23      False\n",
      "2019-06-26    0    0      98      3   23      False\n",
      "2019-06-27    0    0      95      3   23      False\n",
      "2019-06-28    0    0      93      3   24      False\n",
      "2019-06-29    0    0      84      3   25      False\n",
      "2019-06-30    0    0      82      3   25      False\n",
      "2019-07-01    0    0      96      3   24      False\n",
      "2019-07-02    0    0      97      3   23      False\n",
      "2019-07-03    0    0      97      3   22      False\n",
      "2019-07-04    0    0      92      3   22      False\n",
      "2019-07-05    0    0      92      3   23      False\n",
      "2019-07-06    0    0      83      4   25      False\n",
      "2019-07-07    0    0      81      3   25      False\n",
      "2019-07-08    0    0      98      3   24      False\n",
      "2019-07-09    0    0      97      3   23      False\n",
      "2019-07-10    0    0      96      3   23      False\n",
      "2019-07-11    0    0      94      3   23      False\n",
      "2019-07-12    0    0      95      3   24      False\n",
      "2019-07-13    0    0      84      3   26      False\n",
      "2019-07-14    0    0      81      3   25      False\n",
      "2019-07-15    0    1      96      3   24      False\n",
      "2019-07-16    0    1      98      3   24      False\n",
      "2019-07-17    0    1      99      3   24      False\n",
      "2019-07-18    0    1      97      3   24      False\n",
      "2019-07-19    0    0      95      3   25      False\n",
      "2019-07-20    0    0      83      3   26      False\n"
     ]
    }
   ],
   "source": [
    "proxy_list = ['http://50.116.3.101:3128', 'http://157.245.249.43:8080', 'http://172.106.18.152:8080', 'http://209.90.63.108:80']\n",
    "\n",
    "def get_trend(word: list, t_start: str, t_end: str) -> list:\n",
    "    g_trends = TrendReq(tz=360, proxies=proxy_list)\n",
    "    g_trends.build_payload(word, timeframe = t_start+' '+t_end, geo='', gprop='')\n",
    "    trend = g_trends.interest_over_time()\n",
    "    if trend.size > 0:\n",
    "        #trend.columns = ['0', '1', '2']\n",
    "        #trend = trend.columns.get_loc()\n",
    "        print(trend.head())\n",
    "        return trend.iloc[:, :]\n",
    "    else: \n",
    "        print(word+\" request fail.\")\n",
    "        return []\n",
    "\n",
    "# test\n",
    "trends = get_trend(['NFV', 'SDN', \"google\", \"ocean\", \"car\"], '2019-06-20', '2019-07-20')\n",
    "print(trends)\n",
    "# print(np.mean(get_trend('NFV', '2013-06-20', '2013-07-20'), axis=0))\n",
    "# print(np.std(get_trend('NFV', '2013-06-20', '2013-07-20'), axis=0, ddof=1))\n",
    "# print(np.std([1, 2, 3], ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "date\n",
      "2019-06-20     98\n",
      "2019-06-21     96\n",
      "2019-06-22     83\n",
      "2019-06-23     84\n",
      "2019-06-24     98\n",
      "2019-06-25    100\n",
      "2019-06-26     98\n",
      "2019-06-27     95\n",
      "2019-06-28     93\n",
      "2019-06-29     84\n",
      "2019-06-30     82\n",
      "2019-07-01     96\n",
      "2019-07-02     97\n",
      "2019-07-03     97\n",
      "Name: google, dtype: int32\n",
      "date\n",
      "2019-07-04    92\n",
      "2019-07-05    92\n",
      "2019-07-06    83\n",
      "2019-07-07    81\n",
      "2019-07-08    98\n",
      "2019-07-09    97\n",
      "2019-07-10    96\n",
      "2019-07-11    94\n",
      "2019-07-12    95\n",
      "2019-07-13    84\n",
      "2019-07-14    81\n",
      "2019-07-15    96\n",
      "2019-07-16    98\n",
      "2019-07-17    99\n",
      "Name: google, dtype: int32\n",
      "            NFV  SDN  google  ocean  car  isPartial\n",
      "date                                               \n",
      "2019-06-20    0    0      98      3   23      False\n",
      "2019-06-21    0    0      96      3   24      False\n",
      "2019-06-22    0    0      83      3   25      False\n",
      "2019-06-23    0    0      84      3   25      False\n",
      "2019-06-24    0    0      98      3   23      False\n",
      "2019-06-25    0    1     100      3   23      False\n",
      "2019-06-26    0    0      98      3   23      False\n",
      "2019-06-27    0    0      95      3   23      False\n",
      "2019-06-28    0    0      93      3   24      False\n",
      "2019-06-29    0    0      84      3   25      False\n",
      "2019-06-30    0    0      82      3   25      False\n",
      "2019-07-01    0    0      96      3   24      False\n",
      "2019-07-02    0    0      97      3   23      False\n",
      "2019-07-03    0    0      97      3   22      False\n",
      "2019-07-04    0    0      92      3   22      False\n",
      "2019-07-05    0    0      92      3   23      False\n",
      "2019-07-06    0    0      83      4   25      False\n",
      "2019-07-07    0    0      81      3   25      False\n",
      "2019-07-08    0    0      98      3   24      False\n",
      "2019-07-09    0    0      97      3   23      False\n",
      "2019-07-10    0    0      96      3   23      False\n",
      "2019-07-11    0    0      94      3   23      False\n",
      "2019-07-12    0    0      95      3   24      False\n",
      "2019-07-13    0    0      84      3   26      False\n",
      "2019-07-14    0    0      81      3   25      False\n",
      "2019-07-15    0    1      96      3   24      False\n",
      "2019-07-16    0    1      98      3   24      False\n",
      "2019-07-17    0    1      99      3   24      False\n"
     ]
    }
   ],
   "source": [
    "t = trends.iloc[:28, :]\n",
    "mid = int(t.shape[0]/2)\n",
    "print(mid)\n",
    "print(t.iloc[:mid, 2])\n",
    "print(t.iloc[mid:, 2])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_trend(word: str, pub_time: datetime) -> bool:\n",
    "    base_line = 0\n",
    "    \n",
    "    y_start = str(pub_time + timedelta(days=-180)).split(' ')[0]\n",
    "    y_end = str(pub_time + timedelta(days=180)).split(' ')[0]\n",
    "    y_trend = get_trend(word, y_start, y_end)\n",
    "#     print(y_start)\n",
    "#     print(pub_time)\n",
    "#     print(y_end)\n",
    "    if len(y_trend) > 0:\n",
    "        y_mean = np.mean(y_trend)\n",
    "        y_std = np.std(y_trend, ddof=1)\n",
    "\n",
    "        # cal base line\n",
    "        if y_mean+y_std*2 > 100:\n",
    "            base_line = 100\n",
    "        else: base_line = y_mean+y_std*2\n",
    "\n",
    "        for i in range(0, 2):\n",
    "            if y_trend[25+i] >= base_line: return True\n",
    "    return False\n",
    "\n",
    "# Test\n",
    "cats = get_category(df['Page content'][1])\n",
    "print(cats)\n",
    "for cat in cats:\n",
    "    if is_trend(cat, get_date(df['Page content'][1])):\n",
    "        print(\"POP\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(df['Popularity'].values[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "res = []\n",
    "\n",
    "for i in range(100):\n",
    "    print('Step: '+ str(i))\n",
    "    cats = get_category(df['Page content'][i])\n",
    "    flag = True\n",
    "    for cat in cats:\n",
    "        if is_trend(cat, get_date(df['Page content'][i])):\n",
    "            res.append(1)\n",
    "            flag = False\n",
    "            print('1')\n",
    "            break\n",
    "    if flag:\n",
    "        res.append(-1)\n",
    "        print('-1')\n",
    "print('Done')\n",
    "pred = np.array(res)\n",
    "print(pred)\n",
    "print('Accuracy: %.2f' % accuracy_score(pred, np.array(df['Popularity'].values[:100])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "res = []\n",
    "\n",
    "for i in range(20):\n",
    "    print('Step: '+ str(i))\n",
    "    cats = get_title(df['Page content'][i]).split(' ')\n",
    "    print(cats)\n",
    "    flag = True\n",
    "    for cat in cats:\n",
    "        if is_trend(cat, get_date(df['Page content'][i])):\n",
    "            res.append(1)\n",
    "            flag = False\n",
    "            break\n",
    "    if flag:\n",
    "        res.append(-1)\n",
    "print('Done')\n",
    "pred = np.array(res)\n",
    "print(pred)\n",
    "print('Accuracy: %.2f' % accuracy_score(pred, np.array(df['Popularity'].values[:20])))\n",
    "# print('AUC: %.2f' % auc(pred, np.array(df['Popularity'].values[:20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
